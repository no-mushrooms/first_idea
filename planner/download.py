import os
# è®¾ç½®å›½å†…é•œåƒæºï¼ˆå¿…é¡»åœ¨å¯¼å…¥transformersä¹‹å‰ï¼‰
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

# ç›´æ¥ä½¿ç”¨æ¨¡å‹åç§°ï¼ŒHuggingFaceä¼šè‡ªåŠ¨ä»ç¼“å­˜åŠ è½½
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

print("å¼€å§‹ä¸‹è½½ Qwen/Qwen2-7B-Instruct...")
print(f"ä½¿ç”¨é•œåƒ: {os.environ.get('HF_ENDPOINT', 'https://huggingface.co')}")

try:
    print("æ­£åœ¨ä¸‹è½½ tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(
        "Qwen/Qwen2-7B-Instruct",
        trust_remote_code=True
    )
    print("âœ… Tokenizer ä¸‹è½½å®Œæˆ")
    
    print("æ­£åœ¨ä¸‹è½½æ¨¡å‹...")
    model = AutoModelForCausalLM.from_pretrained(
        "Qwen/Qwen2-7B-Instruct",
        torch_dtype=torch.float16,
        trust_remote_code=True
    )
    print("âœ… æ¨¡å‹ä¸‹è½½å®Œæˆ")
    
    print(f"ğŸ‰ æ¨¡å‹å·²ç¼“å­˜åˆ°é»˜è®¤ä½ç½®: ~/.cache/huggingface/hub/")
    
except Exception as e:
    print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")